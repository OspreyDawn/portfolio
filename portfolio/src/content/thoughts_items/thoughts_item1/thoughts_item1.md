# Interface design with The Design of Everyday Things in mind; communicating complex interactions.

#WritingForWritingsSake

When we humans navigate the world around us, we do so through the analysation of all the data that is absorbed by our senses and combine it with our inherent knowledge. There are the usual senses people can identify which are the most obvious like sight, sound, touch, taste and smell but there are an abundance of others that are implicit in the everyday, things like thermoception (the ability to detect temperature) and proprioception (the ability to sense where your body is in space).

As soon as we enter the digital world, we lose almost all of our senses; we are primarily relying on learned knowledge accumulated through past experiences. Vision is our primary sense stimulated through the use of visual displays, sometimes sound is present trhough speakers and even rarer still is the use of touch through vibration. These seem to be the only three senses stimulated in mainstream digital devices. Out of the three, vision is the only sense that permeates all digital experiences that humans encounter on a daily basis; one sense out of many. Not only do interfaces have to have to account for this lack of sensorial input but they have to communicate a visual language that accounts for this. This puts enormous strain on a user's past knowledge to navigate an interface; since the base knowledge of the everyday user varies wildly so does the effective mileage of any given interface.

There's a reason why Apple used skeuomorphism when they first released the iPhone. The visual replication of real, tangible objects in an interface to communicate how one might interact with them in a digital way makes sense; certain physical objects provide affordances that contextual-less digital artefacts can't. Prior to the iPhone, there had been no uniform visual language to relay how one might interact with an interface totally reliant on touch. The Notepad app looked like an actual note pad, with yellow paper pages and stitching, buttons had visual cues to make them look three dimensional that depressed when touched much like one would expect to see in the real world. iOS was brand new at the time so they had to communicate how to navigate the world that they had created in terms that they knew the everyday user would understand.

This is not the first time this has happened either; think of the traditional desktop user interface. Back in the 70's at Xerox PARC, researchers had created the first graphical user interface for the mouse, another invention of theirs, using the same principles. The screen represented an actual desk, data was given names like 'files' and 'folders' which, at the time, had no meaning in a digital sense.  The windows on the screen were the files layered on top each other as one might expect on top of a desk. As time has progressed the language has been learned and streamlined for both the desktop and mobile interfaces, sometimes at the expense of the user. The general approach to designing interfaces acknowledges that users navigate their worlds through their senses and knowledge accumulated from prior experiences remains the same however. How well it is executed can be judged on these terms.

So how do humans interact with the worlds they inhabit, digital or otherwise? First, a user has a goal in mind that they want to achieve. Then, that user has to act upon the world; they have to manipulate something or someone with their goal in mind. Finally, a user must check that the actions they executed resulted in the accomplishment of their goal. This is the basis of how people navigate the world around them that Don Norman describes in greater detail as 'the Seven Stages of Action' in his seminal book 'The Design of Everyday Things.' The seven stages are:

* Forming the Goal
* Forming the Intention
* Specifying an Action
* Executing the Action
* Perceiving the State of the World
* Interpreting the State of the World
* Evaluating the Outcome

This is only an approximate model, but it serves well as a way of evaluating the behaviours of any given user. Sometimes these actions are completed in a matter of seconds, sometimes they occur over longer periods of time. Sometimes they are planned, sometimes they are spontaneous. Other times, completed goals feed into others once one is completed creating subgoals and constant feedback loops. What is certain is that someone, somewhere, is at one of these stages at any one time, one way or another. How easy it is for any given user to achieve their goals can be measured in the gulfs of both execution and evaluation.

___

'Does a system provide actions that correspond to the intentions of a person? The difference between the intentions and allowable actions is the Gulf of Execution...

Does the system provide a physical representation that can be directly perceived and that is directly interpretable in terms of the intentions and expectations of the person? The Gulf of Evaluation reflects the amount of effort that the person must exert to interpret the physical state of the system and to determine how well the expectations and intentions have been met.'

The Design of Everyday Things - Don Norman

___

The more seamless an interface, or system as Norman describes, the easier it is for someone to execute their goal and evaluate a system's effects to determine whether the desirable results are met are what differentiates a well designed interface from a bad one. An interface is what ever a person interacts with to achieve their goal. A door handle is as much an interface as an app's GUI is.

Interfaces can be digital, physical or both; each comes with its own strengths, weaknesses and limitations. The greatest benefit of a digital interface is its flexibility; it is able to change the way it looks and functions based on the context it is in, ideally providing the user the most useful and relevant options at the right time. Flexibility comes with complexity that needs to be carefully managed so that users don't get lost. So how do you manage this complexity? Look to the natural world. It runs by consistent logic that has its own established affordances and laws created over billions of years. These can be used to communicate specific messages in a physical interface and have contributed to human instinct over millennia. Vibrant colours in wildlife naturally draw our attention to warn us; a ripe apple falls from a tree the same way a nut does due to gravity. A digital interface needs to have its own consistent logic or language that people can learn to trust and predict when they execute and evaluate any given action they take. An interface needs it's own language.

The Oxford Dictionary defines language as 'the system of communication used by a particular community.' When you design an interface your community is your users. You are communicating what they can and cannot do, you are providing the tool that allows them to execute and evaluate those actions and language is the tool that establishes these expectations. Design is therefore a language. A good interface language is an elegant language; a language where related actions are grouped, where items of a similar nature can be acted upon in similar, expected ways. The more inconsistent a language is, the more exceptions there are and the harder it is to learn and the more afraid users are to trust it. It's one of the major reasons why English is so hard to learn as a second language. That's why it's often best to piggy back established interface languages and conventions over truly unique ones. To create a new one requires extensive effort teaching it and should only be done if absolutely necessary and even then only sparingly.

An interface language can be inherently complex, especially if you are trying to help a user solve a complex problem. That doesn't mean it has to be difficult to learn. Complexity is not the antithesis to simplicity, they are qualia that define different levels of understanding.  A designer needs to understand a system fully to help a user navigate it with as little effort as possible. That is simplicity. The objective is not to reduce, to a minimum, the number of required actions to be taken. The objective is to help achieve a goal as accurately as possible with the least cognitive stress. Elegance is understanding complexity well enough to lead a user with confidence. An interface should be judged by how elegantly it achieves the users' goals, rather than how simple it appears.

Understanding the complexity of a system that any given problem resides in requires an intimate understanding of it from the view of those most concerned with the problem: the users themselves. Everyone forms some sort of goal when they want to do something, the problem may be they're not quite sure how to achieve it i.e. the gulfs of execution and evaluation are too wide for them to make any headway. To begin solving these problems, we need to understand why that person has fromed that goal in the first place. While everyone has a goal, the nature and intent of those goals are each individual cases that need to be understood. The seven stages of action provide reasonable perspective on 'how' people go about trying to solve their problems but don't provide enough insight into the 'why' and that's where doing a bit of user research comes in.

---

*I'm going to focus on an application of the seven stages of thinking with reference to the work I did for Westpac New Zealand's internet banking app called 'Westpac One,' specifically the task flow for the lock screen.*

In banking apps, the foremost concern of the user is making sure that the app they are using is secure and deliberate. When dealing with a user's possession as prized as their wealth, there is a greater need for discrepancy between a user's intent and what is presented on screen. For Westpac, they have their own processes going on in the background that each have clear purposes for ensuring a user's security that they want to communicate. For the user, they just want to make sure that whatever they're doing, wherever they are, is safe and secure.

When Westpac One was first released, its unique feature was that users could quickly check their balance without having to login. As soon as they wanted to do something that needed to be more secure, like making a payment for example, only then would they need securely unlock their account. By unlocking their account, the Westpac One app goes through a series of checks on the back end that the user doesn't necessarily need to know the specifics of, but are important to know because the checks require time and a connection to the internet to execute. This meant the app has three states: a quick access state, the unlocking state and a logged in state; communicating these spaces and the transitions between them is incredibly important.

First, transparency facilitates trust which is a key factor in any app. Transparency means clearly showing how an action a user takes affects the state of the app clearly. Second, like a person's proprioception, we need to communicate how a user is moving through these spaces so that they don't feel lost; an interface without clear proprioception is like being in a sensory deprivation tank. We need to be able to let the user evaluate the world of the app for every action we let them enact on it. Third, the nature of popular mobile devices these days means we are restricted to only one sense; the visual through the use of the screen. (sound (speaker) and touch (vibration) could have been considered but it wasn't something we explored at the time). So how did we solve for this?

*Note: what follows is an account of what I proposed, not what actually ended up in the final app. Some design decisions are present in the current iteration of the app, others are not.*

Because the user is transitioning to three different spaces over the course of this task flow, it was important that we made clear visual distinctions between the spaces, but they couldn't be so different otherwise users could become disorientated. The quick access state and logged in state both show the same information but have different restrictions on the actions you can do in each. Because they were so similar, we needed to make sure they looked visually similar in layout for continuity's sake. Here's some screenshots:

Key elements, the user accounts, the nav-bar items and their titles, are all in the same place from state to state (the lock icon is in a different place due to technical limitations in the app). The differences are seen in the more subdued palette. The logged in state is removed of distractions, putting a focus on the serious nature of the tasks to be carried out in the logged in state. Contrast to this, the quick access state is more playful representing a more relaxed space to comfort users, reminding them they have access to their account but nothing is going to go wrong in this space. The state of these spaces is also represented by the state of the lock icon. In quick access, the lock is closed signifying the account is secured. Logged in, the lock is open meaning the user is now free to do what they choose with their accounts.

The pad-lock icon is an important element in communicating state and intention of the app's functions. This is where real life metaphors in interface design are at their most useful. A pad-lock in the real world is used to ensure that what ever it protects is kept secure so only those who have the code to it can unlock it; this the affordance the use of the pad-lock icon provides. This sets expectations on the interface element and communicates intention using common knowledge of the real world. It is made distinct in the quick access interface with the use of bold purple to encourage users to tap on it (as is expected in touch interfaces nowadays). When they do, an animation plays to show the change in the space:

Notice how when the user taps on the button, the button itself expands and changes shape to fill the entire screen and reveals a keypad. This shows how the user is transitioning from the quick access space to the unlocking space. With the use of the lock icon, we set the user's expectations for some sort of credential entry and revealing the keypad rewards this expectation.

Animations like this are incredibly useful in communicating a change in state of an interface. There is a clear logical progression that animations provide. They can give context, consistency and history to the elements of an interface. When you move from one room to another, you don't appear in the next room the moment you decide to travel. You stop what you were doing, you get up and walk, you might open a door and so on until you get to your desitination. There is a clear number of steps you take after you execute on intention before you actually get there. It's not just practicality of it either. It provides context, it keeps you orientated and comfortable in the space you are in. In abstract spaces like digital interfaces, animations are a god send. They clearly show how you can move from one place to another not just to know where you are, but show how you got there and where you might go if you want to go back. Users need to be able to evaluate the result of their actions to know if their goals have been met. In the digital realm, animations are the simplest way to provide this feedback. See how when we tap on a key, the numbers on the padlock rotate:

Again we're referring back to the metaphor we used with the padlock. Now we are showing that the PIN keys are tied to the padlock and using them is acting on the padlock meaning we are in the process of unlocking it. There's also an indicator that changes state when we press a key to let the user know that the press was registered and infers how many more key presses we have to go. The key press is the execution, the pad-lock rotates, a key indicator is filled and now the user is able to evaluate their action. When all the keys are entered one of two results will happen:

If the PIN is entered in wrong, the lock will shake and an error message will appear. Inspired by the way Apple animates an incorrect password on macOS, the shaking immediately catches the eye of the user reminiscent of someone shaking their head or the jarring feedback you get when putting in an incorrect key into a lock. It's sharp and intrusive, a stark contrast to the smoothness demonstrated by previous animations. We want to tell the user something went wrong and when things go wrong its usually sudden and unpleasant. Shaking the padlock this way communicates this.

If the PIN is correct, the keypad disappears but that doesn't mean we're at the end. Banks have a myriad of security checks going on in the background but having the loading screens linger on a single animation can convey a sense of slowness, leaving users thinking something has gone wrong when everything is perfectly fine. Having the feeling something has gone wrong when you're dealing with somebody's money is never a feeling you want to convey so showing some sort of progress or representation of something going on behind the scenes can give the user confidence in your app's ability to function.  

Animations in general do a great job of showing how all the different parts of your app all connect together allowing a way to see how all the different screens function and relate to each other, they are the journey between landmarks that a traveller might take when abroad providing crucial orientation. The greatest feature of animations however has to be the amount of transparency they provide. The use of the padlock in the app mockup is a good example of how being transparent about what your app is doing to the user. Being transparent helps the user to understand the interface you are putting them in, which creates expectation, making it predictable. The more predictable an interface and the more consistently it satisfies expectations, the more a user will trust an interface. The more trust there is, the less they will think about the specifics of their actions and the gulfs of execution and evaluation become incredibly narrow; the sign of a elegant interface.

---

Let's not callout Westpac specifically, instead let's use past experiences to influence and demonstrate a point. It can be heavily based on the Westpac design, and I can use the same logic, but by not calling them out explicitly I can bring in some changes that I feel will be better plus it gives me an excuse to create a whole new app prototype to demonstrate prototyping skills. Apply this fix after content complete.
